{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime as dt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns=200\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "path=r\"C:\\Users\\berid\\python\\Shell Stations Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "edge_options = Options()\n",
    "edge_options.add_argument('--headless')\n",
    "edge_options.add_argument(\"--blink-settings=imagesEnabled=false\")  # Disable images\n",
    "\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://find.pshell.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Argentina': 'https://find.shell.com/ar/fuel/locations',\n",
       " 'Australia': 'https://find.shell.com/au/fuel/locations',\n",
       " 'Austria': 'https://find.shell.com/at/fuel/locations',\n",
       " 'Belgium': 'https://find.shell.com/be/fuel/locations',\n",
       " 'Botswana': 'https://find.shell.com/bw/fuel/locations',\n",
       " 'Brazil': 'https://find.shell.com/br/fuel/locations',\n",
       " 'Bulgaria': 'https://find.shell.com/bg/fuel/locations',\n",
       " 'Canada': 'https://find.shell.com/ca/fuel/locations',\n",
       " 'Colombia': 'https://find.shell.com/co/fuel/locations',\n",
       " 'Czechia': 'https://find.shell.com/cz/fuel/locations',\n",
       " 'Denmark': 'https://find.shell.com/dk/fuel/locations',\n",
       " 'Dominican Republic': 'https://find.shell.com/do/fuel/locations',\n",
       " 'Ecuador': 'https://find.shell.com/ec/fuel/locations',\n",
       " 'El Salvador': 'https://find.shell.com/sv/fuel/locations',\n",
       " 'France': 'https://find.shell.com/fr/fuel/locations',\n",
       " 'Germany': 'https://find.shell.com/de/fuel/locations',\n",
       " 'Honduras': 'https://find.shell.com/hn/fuel/locations',\n",
       " 'Hong Kong': 'https://find.shell.com/hk/fuel/locations',\n",
       " 'Hungary': 'https://find.shell.com/hu/fuel/locations',\n",
       " 'India': 'https://find.shell.com/in/fuel/locations',\n",
       " 'Indonesia': 'https://find.shell.com/id/fuel/locations',\n",
       " 'Italy': 'https://find.shell.com/it/fuel/locations',\n",
       " 'Kenya': 'https://find.shell.com/ke/fuel/locations',\n",
       " 'Luxembourg': 'https://find.shell.com/lu/fuel/locations',\n",
       " 'Macao': 'https://find.shell.com/mo/fuel/locations',\n",
       " 'Malaysia': 'https://find.shell.com/my/fuel/locations',\n",
       " 'Mauritius': 'https://find.shell.com/mu/fuel/locations',\n",
       " 'Mexico': 'https://find.shell.com/mx/fuel/locations',\n",
       " 'Morocco': 'https://find.shell.com/ma/fuel/locations',\n",
       " 'Netherlands': 'https://find.shell.com/nl/fuel/locations',\n",
       " 'Norway': 'https://find.shell.com/no/fuel/locations',\n",
       " 'Oman': 'https://find.shell.com/om/fuel/locations',\n",
       " 'Pakistan': 'https://find.shell.com/pk/fuel/locations',\n",
       " 'Philippines': 'https://find.shell.com/ph/fuel/locations',\n",
       " 'Poland': 'https://find.shell.com/pl/fuel/locations',\n",
       " 'Portugal': 'https://find.shell.com/pt/fuel/locations',\n",
       " 'Puerto Rico': 'https://find.shell.com/pr/fuel/locations',\n",
       " 'Singapore': 'https://find.shell.com/sg/fuel/locations',\n",
       " 'Slovakia': 'https://find.shell.com/sk/fuel/locations',\n",
       " 'Slovenia': 'https://find.shell.com/si/fuel/locations',\n",
       " 'South Africa': 'https://find.shell.com/za/fuel/locations',\n",
       " 'Spain': 'https://find.shell.com/es/fuel/locations',\n",
       " 'Switzerland': 'https://find.shell.com/ch/fuel/locations',\n",
       " 'Thailand': 'https://find.shell.com/th/fuel/locations',\n",
       " 'Tunisia': 'https://find.shell.com/tn/fuel/locations',\n",
       " 'Türkiye': 'https://find.shell.com/tr/fuel/locations',\n",
       " 'Uganda': 'https://find.shell.com/ug/fuel/locations',\n",
       " 'United Kingdom': 'https://find.shell.com/gb/fuel/locations',\n",
       " 'United States': 'https://find.shell.com/us/fuel/locations'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_dict={}\n",
    "country_elements=driver.find_elements(By.CSS_SELECTOR,'main[class=\"locations-list\"] li[class=\"geographical-list-item\"] a[class=\"geographical-list-item__link\"]')\n",
    "for country_element in country_elements:\n",
    "    country_url=country_element.get_attribute('href')\n",
    "    country_name=country_element.get_attribute('textContent').split('(')[0].strip()\n",
    "    country_dict[country_name]=country_url\n",
    "country_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state_dict(country_url):\n",
    "    state_dict={}\n",
    "\n",
    "    driver.get(country_url)\n",
    "    #time.sleep(1)\n",
    "\n",
    "    state_elements=driver.find_elements(By.CSS_SELECTOR,'main[class=\"locations-list\"] li[class=\"geographical-list-item\"] a[class=\"geographical-list-item__link\"]')\n",
    "    for i,state_element in enumerate(state_elements,start=1):\n",
    "        try:\n",
    "            state_url=state_element.get_attribute('href')\n",
    "            state_name=state_element.get_attribute('textContent').split('(')[0].strip()\n",
    "            state_name=f'{i}_{state_name}'\n",
    "            state_dict[state_name]=state_url\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_city_dict(country_url):\n",
    "    city_dict={}\n",
    "\n",
    "    driver.get(country_url)\n",
    "    #time.sleep(1)\n",
    "\n",
    "    city_elements=driver.find_elements(By.CSS_SELECTOR,'main[class=\"locations-list\"] li[class=\"geographical-list-item\"] a[class=\"geographical-list-item__link\"]')\n",
    "    for i,city_element in enumerate(city_elements,start=1):\n",
    "        try:\n",
    "            city_url=city_element.get_attribute('href')\n",
    "            city_name=city_element.get_attribute('textContent').split('(')[0].strip()\n",
    "            city_name=f'{i}_{city_name}'\n",
    "            city_dict[city_name]=city_url\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    return city_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_station_url(city_url):\n",
    "\n",
    "    driver.get(city_url)\n",
    "    #time.sleep(1)\n",
    "\n",
    "    station_elements=driver.find_elements(By.CSS_SELECTOR,'main[class=\"locations-list\"] li[class=\"station-list-item\"] a:nth-child(1)')\n",
    "    station_urls=[]\n",
    "    for station_element in station_elements:\n",
    "        try:\n",
    "            station_url=station_element.get_attribute('href')\n",
    "            station_urls.append(station_url)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return station_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'Argentina',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Belgium',\n",
       " 'Botswana',\n",
       " 'Brazil',\n",
       " 'Bulgaria',\n",
       " 'Canada',\n",
       " 'Colombia',\n",
       " 'Czechia',\n",
       " 'Denmark',\n",
       " 'Dominican Republic',\n",
       " 'Ecuador',\n",
       " 'El Salvador',\n",
       " 'France',\n",
       " 'Germany',\n",
       " 'Honduras',\n",
       " 'Hong Kong',\n",
       " 'Hungary',\n",
       " 'India',\n",
       " 'Indonesia',\n",
       " 'Italy',\n",
       " 'Kenya',\n",
       " 'Luxembourg',\n",
       " 'Macao',\n",
       " 'Malaysia',\n",
       " 'Mauritius',\n",
       " 'Mexico',\n",
       " 'Morocco',\n",
       " 'Netherlands',\n",
       " 'Norway',\n",
       " 'Oman',\n",
       " 'Pakistan',\n",
       " 'Philippines',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'Puerto Rico',\n",
       " 'Singapore',\n",
       " 'Slovakia',\n",
       " 'Slovenia',\n",
       " 'South Africa',\n",
       " 'Spain',\n",
       " 'Switzerland',\n",
       " 'Thailand',\n",
       " 'Tunisia',\n",
       " 'Türkiye',\n",
       " 'Uganda',\n",
       " 'United Kingdom',\n",
       " 'United States',\n",
       " 'USA1',\n",
       " 'USA2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_countries=[file.split('_')[0] for file in os.listdir(os.path.join(path,'country_urls')) if '_URLs' in file]\n",
    "scraped_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_station_urls=[]\n",
    "\n",
    "for country_name, country_url in country_dict.items():\n",
    "    if country_name in scraped_countries:\n",
    "        continue\n",
    "\n",
    "    driver.get(country_url)\n",
    "\n",
    "    if driver.find_element(By.CSS_SELECTOR,'main[class=\"locations-list\"] h2[class=\"locations-list__heading\"]').text.replace('\\n','').strip()=='States':\n",
    "        \n",
    "        state_dict=return_state_dict(country_url)\n",
    "        for state_name, state_url in state_dict.items():\n",
    "            city_dict=return_city_dict(state_url)\n",
    "            for city_name, city_url in city_dict.items():\n",
    "                station_urls=return_station_url(city_url)\n",
    "                all_station_urls.append(station_urls)\n",
    "                \n",
    "    elif driver.find_element(By.CSS_SELECTOR,'main[class=\"locations-list\"] h2[class=\"locations-list__heading\"]').text.replace('\\n','').strip()=='Cities':\n",
    "        city_dict=return_city_dict(country_url)\n",
    "        for city_name, city_url in city_dict.items():\n",
    "            station_urls=return_station_url(city_url)\n",
    "            all_station_urls.append(station_urls)\n",
    "    else:\n",
    "        station_urls=return_station_url(country_url)\n",
    "        all_station_urls.append(station_urls)\n",
    "\n",
    "    pickle.dump(all_station_urls,open(os.path.join(path,country_urls,f'{country_name}_URLs.pickle'),'wb'))\n",
    "    all_station_urls=[]\n",
    "    print(country_name,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37964"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls=[]\n",
    "\n",
    "for file in os.listdir(os.path.join(path,'country_urls')):\n",
    "    if '_URLs' in file:\n",
    "        url_lists=pickle.load(open(os.path.join(path,'country_urls',file),'rb'))\n",
    "        for url_list in url_lists:\n",
    "            for url in url_list:\n",
    "                all_urls.append(url)\n",
    "\n",
    "\n",
    "all_urls=list(set(all_urls))\n",
    "all_urls=[url for url in all_urls if 'https' in str(url)]\n",
    "\n",
    "pickle.dump(all_urls,open(os.path.join(path,'country_urls',f'all_URLs.pickle'),'wb'))\n",
    "\n",
    "len(all_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls=pickle.load(open(os.path.join(path,'country_urls', f'all_URLs.pickle'),'rb'))\n",
    "all_urls=sorted(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37964"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dict(url):\n",
    "    driver.get(url)\n",
    "    html=driver.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    try:\n",
    "        fuels='New Line'.join([i.get_text(separator='|') for i in soup.select('table[class=\"station-page-fuel-prices__table\"] tr')])\n",
    "    except:\n",
    "        fuels=None\n",
    "\n",
    "    try:\n",
    "        ev_charging=soup.select_one('section[class=\"section-with-title station-page-ev-charging\"]').get_text(separator='|')\n",
    "    except:\n",
    "        ev_charging=None\n",
    "\n",
    "    try:\n",
    "        hydrogen=soup.select_one('section[class=\"section-with-title section-with-title--hydrogen\"]').get_text(separator='|')\n",
    "    except:\n",
    "        hydrogen=None\n",
    "\n",
    "    try:\n",
    "        opening_hours='New Line'.join([i.get_text(separator='|') for i in soup.select('table[class=\"opening-hours__table opening-hours__table--single\"] tr')])\n",
    "    except:\n",
    "        opening_hours=None\n",
    "\n",
    "    try:\n",
    "        services='|'.join([i.text for i in soup.select('section[id=\"features\"] li')])\n",
    "    except:\n",
    "        services=None\n",
    "\n",
    "    try:\n",
    "        more_at_location='|'.join([i.text for i in soup.select('section[id=\"more_at_location\"] li')])\n",
    "    except:\n",
    "        more_at_location=None\n",
    "\n",
    "    try:\n",
    "        about_the_station=soup.select_one('article[class=\"station-page-about__text\"]').text\n",
    "    except:\n",
    "        about_the_station=None\n",
    "\n",
    "    try:\n",
    "        address=soup.select_one('section[id=\"details\"]').text\n",
    "    except:\n",
    "        address=None\n",
    "\n",
    "    try:\n",
    "        country=soup.select_one('div[class=\"breadcrumbs__links\"]').get_text(separator='|')\n",
    "    except:\n",
    "        country=None\n",
    "\n",
    "    dict={'URL':url,'Country, State, City':country,'Fuels':fuels,'EV':ev_charging,'Hydrogen':hydrogen,'Opening Hours':opening_hours,'Services':services,'More At Location':more_at_location,'About':about_the_station,'Address':address}\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs scraped: 37955\n"
     ]
    }
   ],
   "source": [
    "all_stations_data = []\n",
    "for file in os.listdir(os.path.join(path, 'all_data')):\n",
    "    if 'all_' in file:\n",
    "        data = pickle.load(open(os.path.join(path, 'all_data', file), 'rb'))\n",
    "        for item in data:\n",
    "            all_stations_data.append(item)\n",
    "\n",
    "all_stations_data=[dict for dict in all_stations_data if dict['Address']]\n",
    "\n",
    "\n",
    "# Filter entries based on 'Address' and remove duplicates based on 'URL'\n",
    "unique_urls = set()\n",
    "filtered_data = []\n",
    "for entry in all_stations_data:\n",
    "    if 'URL' in entry and entry['URL'] not in unique_urls:\n",
    "        filtered_data.append(entry)\n",
    "        unique_urls.add(entry['URL'])\n",
    "\n",
    "# Save the filtered data\n",
    "pickle.dump(filtered_data, open(os.path.join(path, 'all_data', 'all_stations_data.pickle'), 'wb'))\n",
    "\n",
    "# Print the number of unique URLs scraped\n",
    "scraped_urls = [entry['URL'] for entry in filtered_data]\n",
    "print(f'Number of URLs scraped: {len(scraped_urls)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_to_be_scraped=[url for url in all_urls if url not in scraped_urls]\n",
    "len(urls_to_be_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs Scraped : 9\r"
     ]
    }
   ],
   "source": [
    "all_data=[]\n",
    "\n",
    "for i,url in enumerate(urls_to_be_scraped,start=1):\n",
    "\n",
    "    #if url in scraped_urls:\n",
    "    #    continue\n",
    "\n",
    "    try:\n",
    "        dict=return_dict(url)\n",
    "        all_data.append(dict)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    if i%1000==0 or i==len(urls_to_be_scraped):\n",
    "        pickle.dump(all_data,open(os.path.join(path,'all_data',f'all_data_{i}'),'wb'))\n",
    "        all_data=[]\n",
    "\n",
    "    print(f'URLs Scraped : {i}',end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(filtered_data)\n",
    "df.to_csv(os.path.join(path,'Shell_Gas_Stations (raw).csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lat']=df['Address'].apply(lambda x: x.split('Lat / Lng')[-1].split('|')[-1].split(',')[0] if isinstance(x,str) else x).apply(pd.to_numeric,errors='coerce')\n",
    "df['Lon']=df['Address'].apply(lambda x: x.split('Lat / Lng')[-1].split('|')[-1].split(',')[1] if isinstance(x,str) else x).apply(pd.to_numeric,errors='coerce')\n",
    "\n",
    "df=df.drop(columns='Address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "country=df['Country, State, City'].apply(lambda x:x.split('|')[1].strip() if isinstance(x,str) else None)\n",
    "state=df['Country, State, City'].apply(lambda x:x.split('|')[2].strip() if isinstance(x,str) and len(x.split('|'))==5 else None)\n",
    "city=df['Country, State, City'].apply(lambda x:x.split('|')[3].strip() if isinstance(x,str) and len(x.split('|'))==5 else \n",
    "                                 x.split('|')[2].strip() if isinstance(x,str) and len(x.split('|'))==4 else None)\n",
    "station=df['Country, State, City'].apply(lambda x:x.split('|')[-1].strip() if isinstance(x,str) else None)\n",
    "\n",
    "df.insert(df.columns.get_loc('Country, State, City')+1,'Country',country)\n",
    "df.insert(df.columns.get_loc('Country, State, City')+2,'State',state)\n",
    "df.insert(df.columns.get_loc('Country, State, City')+3,'City',city)\n",
    "df.insert(df.columns.get_loc('Country, State, City')+4,'Station',station)\n",
    "\n",
    "df=df.drop(columns='Country, State, City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuels=df['Fuels'].apply(lambda x:x.split('New Line') if 'New Line' in str(x) else x.split('|')[0] if 'New Line' not in str(x) else None)\n",
    "\n",
    "new_fuel_series=[]\n",
    "for list in fuels:\n",
    "    new_list=[]\n",
    "    for value in list:\n",
    "        fuel_name=value.split('|')[0].strip()\n",
    "        fuel_price=value.split('|')[-1].strip()\n",
    "        dict={'Fuel':fuel_name,'Price':fuel_price}\n",
    "        new_list.append(dict)\n",
    "    new_fuel_series.append(new_list)\n",
    "\n",
    "df['Fuels']=new_fuel_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_pints=df['EV'].apply(lambda x:\n",
    "                                 0 if 'No EV charging information available' in str(x) else\n",
    "                                 x.split('|')[1].split(' ')[0].strip() if x else None\n",
    "                                 ).apply(pd.to_numeric,errors='coerce')\n",
    "df.insert(df.columns.get_loc('EV')+1,'EV Charging Points',ev_pints)\n",
    "df=df.drop(columns='EV')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hydrogen']=df['Hydrogen'].apply(lambda x:'No' if 'No Hydrogen information available' in str(x) else 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours=df['Opening Hours'].apply(lambda x:x.replace('Forecourt','').split('New Line')[1:] if 'New Line' in str(x) else x.split('|')[0])\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "time_format = \"%H:%M\"\n",
    "\n",
    "\n",
    "new_hour_series=[]\n",
    "for list in hours:\n",
    "    new_list=[]\n",
    "    for value in list:\n",
    "        weekday=value.split('|')[0].strip()\n",
    "        try:\n",
    "            hours=value.split('|')[-1].strip()\n",
    "            start_hour=hours.split('-')[0].strip()\n",
    "            start_hour=datetime.strptime(start_hour, time_format)\n",
    "            end_hour=hours.split('-')[1].strip()\n",
    "            end_hour=datetime.strptime(end_hour, time_format)\n",
    "\n",
    "            if end_hour < start_hour:\n",
    "                end_hour += timedelta(days=1)\n",
    "            difference = round((end_hour - start_hour).total_seconds() / 3600)\n",
    "        except:\n",
    "            difference=None\n",
    "\n",
    "\n",
    "        dict={'Weekday':weekday,'Hours':difference}\n",
    "        new_list.append(dict)\n",
    "\n",
    "    new_hour_series.append(new_list)\n",
    "    \n",
    "df['Opening Hours']=new_hour_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Services']=df['Services'].apply(lambda x:'|'.join(sorted([i.strip() for i in x.split('|')])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['More At Location']=df['More At Location'].apply(lambda x:'|'.join(sorted([i.strip() for i in x.split('|')])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns='About')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(os.path.join(path,'Shell_Gas_Stations (cleaned).csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Shell_Gas_Stations (cleaned).csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
